{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecf3d0f-f3b9-4295-a151-ee610a90cf75",
   "metadata": {
    "id": "4ecf3d0f-f3b9-4295-a151-ee610a90cf75"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e164ddb-2907-43e2-bf83-fb18c0712745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4729,
     "status": "ok",
     "timestamp": 1732916196043,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "8e164ddb-2907-43e2-bf83-fb18c0712745",
    "outputId": "31a6b9f0-19fb-4b3a-82a7-8c76a807d4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import Subset\n",
    "from transformers import BertConfig, BertTokenizer, BertForPreTraining\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainingHeads\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainerCallback\n",
    "! pip install datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "dir_path = '/scratch/mmt515'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874388c-186b-439d-9ba1-c5f076716315",
   "metadata": {
    "id": "5874388c-186b-439d-9ba1-c5f076716315",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define the Custom Model Class with Regression Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lHMvScZZdezN",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732915213862,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "lHMvScZZdezN"
   },
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainingHeads\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "@dataclass\n",
    "class BertForPreTrainingWithRegressionOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    regression_logits: torch.FloatTensor = None\n",
    "    masked_lm_loss: Optional[torch.FloatTensor] = None\n",
    "    regression_loss: Optional[torch.FloatTensor] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "class BertForPreTrainingWithRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Initialize BertModel with add_pooling_layer=True\n",
    "        self.bert = BertModel(config, add_pooling_layer=True)\n",
    "        # MLM and NSP heads\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        # Regression head\n",
    "        self.regression_head = nn.Linear(config.hidden_size, 1)\n",
    "        # Initialize regression head weights separately to avoid overwriting pre-trained weights\n",
    "        self.regression_head.apply(self._init_weights)\n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,  # MLM labels\n",
    "        next_sentence_label=None,  # NSP labels (not used in your case)\n",
    "        expression=None,  # Regression labels\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Get outputs from BertModel\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,  # Ensure outputs is a ModelOutput\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
    "        pooled_output = outputs.pooler_output       # (batch_size, hidden_size)\n",
    "\n",
    "        # MLM and NSP predictions\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        # Regression prediction\n",
    "        regression_logits = self.regression_head(pooled_output).squeeze(-1)  # (batch_size)\n",
    "\n",
    "        # Initialize losses\n",
    "        total_loss = None\n",
    "        masked_lm_loss = None\n",
    "        regression_loss = None\n",
    "\n",
    "        # Compute MLM loss if labels are provided\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            total_loss = masked_lm_loss\n",
    "\n",
    "        # Compute regression loss if expression labels are provided\n",
    "        if expression is not None:\n",
    "            loss_fct = MSELoss()\n",
    "            regression_loss = loss_fct(regression_logits, expression)\n",
    "            total_loss = total_loss + regression_loss if total_loss is not None else regression_loss\n",
    "\n",
    "        # Return outputs using the custom ModelOutput class\n",
    "        return BertForPreTrainingWithRegressionOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            regression_logits=regression_logits,\n",
    "            masked_lm_loss=masked_lm_loss,\n",
    "            regression_loss=regression_loss,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13be62-d954-4e54-91a6-f88fe998163f",
   "metadata": {
    "id": "3d13be62-d954-4e54-91a6-f88fe998163f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0d3ad-e09a-4f15-8dab-106d947acc41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1732915216999,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "a2d0d3ad-e09a-4f15-8dab-106d947acc41",
    "outputId": "d4ecebdf-498c-4e12-f8f8-ff23d705dcaf"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Paths to tokenizer and model\n",
    "model_path = f\"{dir_path}/local_CodonBERT_Melina/ScCodonBERT\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model using the custom class\n",
    "model = BertForPreTrainingWithRegression.from_pretrained(\n",
    "    model_path,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    from_tf = True\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bP2b59SDT6M3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732904822304,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "bP2b59SDT6M3",
    "outputId": "894284c3-7070-46a9-e099-8e41dd6bc6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForPreTrainingWithRegression(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(101, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(1024, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertPreTrainingHeads(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=101, bias=True)\n",
      "    )\n",
      "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      "  (regression_head): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d491c0d-c9ce-4ecd-8ca5-1a6d7e3620ec",
   "metadata": {
    "id": "7d491c0d-c9ce-4ecd-8ca5-1a6d7e3620ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Full Data (Ground Truth DNA Sequences and Expression Values)\n",
    "* Some re-naming just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908e0e6-e76a-47f6-8432-22eda9aa1539",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1732915218287,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "d908e0e6-e76a-47f6-8432-22eda9aa1539",
    "outputId": "7f2bb290-1ee4-4153-f8e4-5191a141d08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "  mORF name  expression  categorical_te  \\\n",
      "0   YAL067C   -2.173141               1   \n",
      "1   YAL063C   -3.248648               1   \n",
      "2   YAL054C   -2.935977               1   \n",
      "3   YAL049C    0.727998               8   \n",
      "4   YAL047C   -0.159598               5   \n",
      "\n",
      "                                        dna_sequence  \\\n",
      "0  ATGTATTCAATTGTTAAAGAGATTATTGTAGATCCTTACAAAAGAC...   \n",
      "1  ATGTCTCTGGCACATTATTGTTTACTACTAGCCATCGTCACATTGC...   \n",
      "2  ATGTCGCCCTCTGCCGTACAATCATCAAAACTAGAAGAACAGTCAA...   \n",
      "3  ATGGCATCTAATCAACCTGGCAAGTGTTGCTTTGAAGGAGTTTGTC...   \n",
      "4  ATGGTACGTCGATGGATTCCTAGTGGCAGGCATCTTCGCAATAATG...   \n",
      "\n",
      "                                    Protein sequence  \n",
      "0  MYSIVKEIIVDPYKRLKWGFIPVKRQVEDLPDDLNSTEIVTISNSI...  \n",
      "1  MSLAHYCLLLAIVTLLGLTNVVSATTAACLPANSRKNGMNVNFYQY...  \n",
      "2  MSPSAVQSSKLEEQSSEIDKLKAKMSQSAATAQQKKEHEYEHLTSV...  \n",
      "3  MASNQPGKCCFEGVCHDGTPKGRREEIFGLDTYAAGSTSPKEKVIV...  \n",
      "4  MVRRWIPSGRHLRNNDNTGDDDDSEFTNSMDSGMSIPSLRDSMTTR...  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(f'{dir_path}/ScCodonBERT/Sc_cDNA_pep_TE30.csv')\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Verify and rename columns for consistency\n",
    "df.rename(columns={'te30  (log2)': 'expression', 'DNA sequence': 'dna_sequence', 'Categorical TE': 'categorical_te'}, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeae038-9477-4534-94d7-ea3ebec3a509",
   "metadata": {
    "id": "caeae038-9477-4534-94d7-ea3ebec3a509",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pre-process data\n",
    "* Create a TE token from the category of translation efficiency (i.e. TE+category = \"TE10\"\n",
    "* Convert DNA to RNA\n",
    "* Split RNA sequence into codons\n",
    "* make a columns \"sequence_tokens\" that contains the TE and the codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c26d11-5455-4be1-a815-1dfe0ed9893c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1732915224339,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "51c26d11-5455-4be1-a815-1dfe0ed9893c",
    "outputId": "56f68de9-59a7-4865-f5c6-112fe8d55650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA sequences converted to RNA.\n",
      "RNA sequences split into codons.\n",
      "TE tokens combined with codons to form input sequences.\n"
     ]
    }
   ],
   "source": [
    "# Create TE tokens\n",
    "df['TE_token'] = 'TE' + df['categorical_te'].astype(str)\n",
    "\n",
    "# Convert DNA sequences to RNA sequences\n",
    "df['rna_sequence'] = df['dna_sequence'].str.upper().str.replace('T', 'U')\n",
    "\n",
    "print(\"DNA sequences converted to RNA.\")\n",
    "\n",
    "def split_into_codons(sequence):\n",
    "    # Split the sequence into codons of length 3\n",
    "    codons = [sequence[i:i+3] for i in range(0, len(sequence), 3) if len(sequence[i:i+3]) == 3]\n",
    "    return codons\n",
    "\n",
    "# Apply the function to create a new column with codons\n",
    "df['codons'] = df['rna_sequence'].apply(split_into_codons)\n",
    "\n",
    "print(\"RNA sequences split into codons.\")\n",
    "\n",
    "# Combine TE tokens with codons\n",
    "df['sequence_tokens'] = df.apply(lambda row: [row['TE_token']] + row['codons'], axis=1)\n",
    "\n",
    "print(\"TE tokens combined with codons to form input sequences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0eab4-ff60-43f6-90f7-b255fc03fa70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732507589610,
     "user": {
      "displayName": "Antonio Vela G.",
      "userId": "02403341420005464923"
     },
     "user_tz": 300
    },
    "id": "5bd0eab4-ff60-43f6-90f7-b255fc03fa70",
    "outputId": "cff0007d-dcc6-4647-bf70-3260c58b0dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mORF name  expression  categorical_te  \\\n",
      "0   YAL067C   -2.173141               1   \n",
      "1   YAL063C   -3.248648               1   \n",
      "2   YAL054C   -2.935977               1   \n",
      "3   YAL049C    0.727998               8   \n",
      "4   YAL047C   -0.159598               5   \n",
      "\n",
      "                                        dna_sequence  \\\n",
      "0  ATGTATTCAATTGTTAAAGAGATTATTGTAGATCCTTACAAAAGAC...   \n",
      "1  ATGTCTCTGGCACATTATTGTTTACTACTAGCCATCGTCACATTGC...   \n",
      "2  ATGTCGCCCTCTGCCGTACAATCATCAAAACTAGAAGAACAGTCAA...   \n",
      "3  ATGGCATCTAATCAACCTGGCAAGTGTTGCTTTGAAGGAGTTTGTC...   \n",
      "4  ATGGTACGTCGATGGATTCCTAGTGGCAGGCATCTTCGCAATAATG...   \n",
      "\n",
      "                                    Protein sequence TE_token  \\\n",
      "0  MYSIVKEIIVDPYKRLKWGFIPVKRQVEDLPDDLNSTEIVTISNSI...      TE1   \n",
      "1  MSLAHYCLLLAIVTLLGLTNVVSATTAACLPANSRKNGMNVNFYQY...      TE1   \n",
      "2  MSPSAVQSSKLEEQSSEIDKLKAKMSQSAATAQQKKEHEYEHLTSV...      TE1   \n",
      "3  MASNQPGKCCFEGVCHDGTPKGRREEIFGLDTYAAGSTSPKEKVIV...      TE8   \n",
      "4  MVRRWIPSGRHLRNNDNTGDDDDSEFTNSMDSGMSIPSLRDSMTTR...      TE5   \n",
      "\n",
      "                                        rna_sequence  \\\n",
      "0  AUGUAUUCAAUUGUUAAAGAGAUUAUUGUAGAUCCUUACAAAAGAC...   \n",
      "1  AUGUCUCUGGCACAUUAUUGUUUACUACUAGCCAUCGUCACAUUGC...   \n",
      "2  AUGUCGCCCUCUGCCGUACAAUCAUCAAAACUAGAAGAACAGUCAA...   \n",
      "3  AUGGCAUCUAAUCAACCUGGCAAGUGUUGCUUUGAAGGAGUUUGUC...   \n",
      "4  AUGGUACGUCGAUGGAUUCCUAGUGGCAGGCAUCUUCGCAAUAAUG...   \n",
      "\n",
      "                                              codons  \\\n",
      "0  [AUG, UAU, UCA, AUU, GUU, AAA, GAG, AUU, AUU, ...   \n",
      "1  [AUG, UCU, CUG, GCA, CAU, UAU, UGU, UUA, CUA, ...   \n",
      "2  [AUG, UCG, CCC, UCU, GCC, GUA, CAA, UCA, UCA, ...   \n",
      "3  [AUG, GCA, UCU, AAU, CAA, CCU, GGC, AAG, UGU, ...   \n",
      "4  [AUG, GUA, CGU, CGA, UGG, AUU, CCU, AGU, GGC, ...   \n",
      "\n",
      "                                     sequence_tokens  \n",
      "0  [TE1, AUG, UAU, UCA, AUU, GUU, AAA, GAG, AUU, ...  \n",
      "1  [TE1, AUG, UCU, CUG, GCA, CAU, UAU, UGU, UUA, ...  \n",
      "2  [TE1, AUG, UCG, CCC, UCU, GCC, GUA, CAA, UCA, ...  \n",
      "3  [TE8, AUG, GCA, UCU, AAU, CAA, CCU, GGC, AAG, ...  \n",
      "4  [TE5, AUG, GUA, CGU, CGA, UGG, AUU, CCU, AGU, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c1a20-e828-40c4-a2c0-dc6c4c6ff6d4",
   "metadata": {
    "id": "1b5c1a20-e828-40c4-a2c0-dc6c4c6ff6d4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4792cc6e-25ff-470f-a900-35353f70edb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1732915228630,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "4792cc6e-25ff-470f-a900-35353f70edb8",
    "outputId": "41383f6f-0534-4a35-ca8c-f944011a53cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4271\n",
      "Validation samples: 534\n",
      "Test samples: 534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and temp datasets (temp will be split into validation and test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split temp into validation and test datasets\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8a774-5f91-45bc-a149-62d189a944f7",
   "metadata": {
    "id": "c1c8a774-5f91-45bc-a149-62d189a944f7"
   },
   "source": [
    "### Create Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0572c2a-08a0-4e62-982a-8ebb4eb9fbc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1732915231467,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "e0572c2a-08a0-4e62-982a-8ebb4eb9fbc6",
    "outputId": "da1d5650-1ccd-49be-dcf0-f0926c1dba3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created.\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"Datasets created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460cd28-a40f-44bf-b931-3153d701f32e",
   "metadata": {
    "id": "b460cd28-a40f-44bf-b931-3153d701f32e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define Custom Masking Function\n",
    "* Use mapping of codon to amino acid\n",
    "* Define custom masking function that:\n",
    "    * Tokenizes without special characters (no addition of CLS, SEP, or PAD)\n",
    "    * Uses first token (always TE) as CLS token (just by it being the first one, but also by setting its loss label to -100 so that it does not contribute to the MLM\n",
    "    * Truncates proteins longer than 510 amino acids (BERT only takes 512 tokens)\n",
    "    * Adds SEP token at the end of the sequence\n",
    "    * Adds PADs if neccesary\n",
    "    * Masks the TE token with prob masking_prob_TE\n",
    "    * Masks codons with probability masking_prob_codons\n",
    "    * Creates label tensors for training (tensors with ground thruth of masks and PAD everywhere else)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5d222f-073b-4268-a783-9ee91f20e463",
   "metadata": {
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1732915233026,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "fe5d222f-073b-4268-a783-9ee91f20e463"
   },
   "outputs": [],
   "source": [
    "# Define the codon to amino acid mapping (Standard Genetic Code, RNA)\n",
    "codon_to_aa = {\n",
    "    'UUU': 'PHE', 'UUC': 'PHE', 'UUA': 'LEU', 'UUG': 'LEU',\n",
    "    'CUU': 'LEU', 'CUC': 'LEU', 'CUA': 'LEU', 'CUG': 'LEU',\n",
    "    'AUU': 'ILE', 'AUC': 'ILE', 'AUA': 'ILE', 'AUG': 'MET',\n",
    "    'GUU': 'VAL', 'GUC': 'VAL', 'GUA': 'VAL', 'GUG': 'VAL',\n",
    "    'UCU': 'SER', 'UCC': 'SER', 'UCA': 'SER', 'UCG': 'SER',\n",
    "    'CCU': 'PRO', 'CCC': 'PRO', 'CCA': 'PRO', 'CCG': 'PRO',\n",
    "    'ACU': 'THR', 'ACC': 'THR', 'ACA': 'THR', 'ACG': 'THR',\n",
    "    'GCU': 'ALA', 'GCC': 'ALA', 'GCA': 'ALA', 'GCG': 'ALA',\n",
    "    'UAU': 'TYR', 'UAC': 'TYR', 'UAA': 'STP', 'UAG': 'STP',\n",
    "    'CAU': 'HIS', 'CAC': 'HIS', 'CAA': 'GLN', 'CAG': 'GLN',\n",
    "    'AAU': 'ASN', 'AAC': 'ASN', 'AAA': 'LYS', 'AAG': 'LYS',\n",
    "    'GAU': 'ASP', 'GAC': 'ASP', 'GAA': 'GLU', 'GAG': 'GLU',\n",
    "    'UGU': 'CYS', 'UGC': 'CYS', 'UGA': 'STP', 'UGG': 'TRP',\n",
    "    'CGU': 'ARG', 'CGC': 'ARG', 'CGA': 'ARG', 'CGG': 'ARG',\n",
    "    'AGU': 'SER', 'AGC': 'SER', 'AGA': 'ARG', 'AGG': 'ARG',\n",
    "    'GGU': 'GLY', 'GGC': 'GLY', 'GGA': 'GLY', 'GGG': 'GLY'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e086477-c533-4211-a45a-0d8ad0dfc019",
   "metadata": {
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1732915236219,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "4e086477-c533-4211-a45a-0d8ad0dfc019"
   },
   "outputs": [],
   "source": [
    "def custom_masking(examples, masking_prob_codons, masking_prob_TE=0.0):\n",
    "    sequences = examples['sequence_tokens']\n",
    "    expressions = examples['expression']\n",
    "    inputs = []\n",
    "    max_length = 512\n",
    "    max_seq_length = max_length - 1  # Reserve space for [SEP]\n",
    "\n",
    "    for seq_tokens, expr in zip(sequences, expressions):\n",
    "        input_seq = []\n",
    "        labels_seq = []\n",
    "        seq_tokens_upper = [token.upper() for token in seq_tokens]\n",
    "        # The first token is the TE token\n",
    "        TE_token = seq_tokens_upper[0]\n",
    "        # Decide whether to mask the TE token\n",
    "        if np.random.rand() < masking_prob_TE:\n",
    "            # Mask the TE token by replacing it with 'TE0'\n",
    "            input_seq.append('TE0')\n",
    "        else:\n",
    "            input_seq.append(TE_token)\n",
    "        # **Always set the label for TE token to -100**\n",
    "        labels_seq.append(-100)\n",
    "        # Process the rest of the tokens (codons)\n",
    "        for codon in seq_tokens_upper[1:]:\n",
    "            if len(codon) != 3:\n",
    "                input_seq.append(codon)\n",
    "                labels_seq.append(-100)\n",
    "                continue\n",
    "            if np.random.rand() < masking_prob_codons:\n",
    "                aa = codon_to_aa.get(codon, None)\n",
    "                if aa and tokenizer.convert_tokens_to_ids(aa) != tokenizer.unk_token_id:\n",
    "                    input_seq.append(aa)\n",
    "                    codon_id = tokenizer.convert_tokens_to_ids(codon)\n",
    "                    labels_seq.append(codon_id)\n",
    "                else:\n",
    "                    input_seq.append(codon)\n",
    "                    labels_seq.append(-100)\n",
    "            else:\n",
    "                input_seq.append(codon)\n",
    "                labels_seq.append(-100)\n",
    "        # Truncate the input sequence and labels if necessary\n",
    "        if len(input_seq) > max_seq_length:\n",
    "            input_seq = input_seq[:max_seq_length]\n",
    "            labels_seq = labels_seq[:max_seq_length]\n",
    "        # Manually add the [SEP] token\n",
    "        sep_token_id = tokenizer.sep_token_id\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_seq)\n",
    "        input_ids.append(sep_token_id)\n",
    "        labels_seq.append(-100)  # For [SEP]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        # Pad sequences to max_length\n",
    "        sequence_length = len(input_ids)\n",
    "        if sequence_length < max_length:\n",
    "            padding_length = max_length - sequence_length\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            labels_seq += [-100] * padding_length\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        labels_tensor = torch.tensor(labels_seq)\n",
    "        # Prepare the final input dictionary\n",
    "        input_encoding = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels_tensor,\n",
    "            'expression': torch.tensor(expr, dtype=torch.float)\n",
    "        }\n",
    "        inputs.append(input_encoding)\n",
    "    # Convert list of dictionaries to a dictionary of lists\n",
    "    batch = {}\n",
    "    for key in inputs[0]:\n",
    "        batch[key] = torch.stack([inp[key] for inp in inputs])\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "658babe0-13f9-43f6-ad0f-8ecd4d49c1c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1732915244796,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "658babe0-13f9-43f6-ad0f-8ecd4d49c1c1",
    "outputId": "fc5d1ffe-c5b6-48e6-b4d9-a21cac6a8cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Input Tokens: ['TE1', 'AUG', 'UAU', 'UCA', 'AUU', 'GUU', 'AAA', 'GAG', 'AUU', 'AUU', 'GUA', 'GAU', 'CCU', 'UAC', 'AAA', 'AGA', 'CUA', 'LYS', 'UGG', 'GLY', 'UUU', 'AUU', 'CCA', 'GUA', 'AAG', 'CGG', 'CAG', 'GUG', 'GAA', 'ASP', 'CUG', 'CCA', 'GAU', 'GAC', 'UUA', 'AAU', 'UCA', 'ACA', 'GAA', 'AUU', 'GUC', 'ACU', 'AUC', 'UCC', 'AAC', 'AGU', 'ILE', 'CAG', 'AGU', 'CAU', 'GAA', 'ACA', 'GCU', 'GAA', 'AAU', 'UUC', 'AUC', 'THR', 'ACU', 'THR', 'AGU', 'GAA', 'AAA', 'ASP', 'CAA', 'CUA', 'CAU', 'UUU', 'GAG', 'ACU', 'SER', 'AGC', 'UAU', 'AGU', 'GLU', 'HIS', 'AAA', 'GAC', 'AAU', 'GUG', 'AAC', 'GUU', 'ACU', 'AGA', 'AGU', 'UAU', 'GAA', 'UAU', 'AGA', 'GAU', 'GAA', 'GCC', 'ASP', 'ARG', 'CCA', 'TRP', 'UGG', 'AGA', 'UUU', 'PHE', 'GAU', 'GLU', 'GLN', 'GAG', 'UAU', 'CGG', 'AUC', 'AAU', 'GAA', 'AAG', 'GAA', 'AGA', 'UCU', 'CAC', 'AAU', 'AAA', 'UGG', 'UAU', 'SER', 'TRP', 'UUC', 'LYS', 'GLN', 'GGU', 'ACC', 'UCU', 'UUC', 'AAA', 'GAA', 'AAA', 'AAA', 'UUA', 'UUA', 'AUU', 'AAA', 'UUG', 'ASP', 'GUC', 'LEU', 'UUA', 'GCC', 'UUU', 'UAU', 'SER', 'UGU', 'AUU', 'GCU', 'UAU', 'UGG', 'GUG', 'AAA', 'UAU', 'LEU', 'GAU', 'ACG', 'GUU', 'AAU', 'AUA', 'AAC', 'AAC', 'GCU', 'UAC', 'GUU', 'UCG', 'GGA', 'AUG', 'AAG', 'GAA', 'GAU', 'UUA', 'GGC', 'UUU', 'CAA', 'GGU', 'ASN', 'GAU', 'UUG', 'GUG', 'CAU', 'ACU', 'CAA', 'GUA', 'AUG', 'TYR', 'ACA', 'GUU', 'GGU', 'AAU', 'AUU', 'AUA', 'UUU', 'CAA', 'UUG', 'CCA', 'UUU', 'UUG', 'AUU', 'UAC', 'LEU', 'AAC', 'AAG', 'CUC', 'CCA', 'UUA', 'AAC', 'UAU', 'GUU', 'UUA', 'CCA', 'AGC', 'CUC', 'GAC', 'UUA', 'UGU', 'UGG', 'UCG', 'CUU', 'UUA', 'ACC', 'GUU', 'GGU', 'GCU', 'GCA', 'TYR', 'GUC', 'AAU', 'UCU', 'GUA', 'CCA', 'CAC', 'UUG', 'AAA', 'GCA', 'ILE', 'ARG', 'UUU', 'UUC', 'AUU', 'GGG', 'GCU', 'UUU', 'GAA', 'GCG', 'CCA', 'AGU', 'UAU', 'UUG', 'GCA', 'UAC', 'CAA', 'UAU', 'UUG', 'UUU', 'GGU', 'UCC', 'UUU', 'UAC', 'AAA', 'CAU', 'GAU', 'GAA', 'AUG', 'GUG', 'CGU', 'CGU', 'UCU', 'GCU', 'UUU', 'UAC', 'UAU', 'LEU', 'GGC', 'CAG', 'UAU', 'AUC', 'GGU', 'AUU', 'CUA', 'UCC', 'GCU', 'GGU', 'GGG', 'AUC', 'CAG', 'UCA', 'GCC', 'GUA', 'UAU', 'UCA', 'UCG', 'UUA', 'AAU', 'GGU', 'GUA', 'ASN', 'GGU', 'UUA', 'GAG', 'GGA', 'UGG', 'ARG', 'UGG', 'AAC', 'PHE', 'AUU', 'AUU', 'GAC', 'GCU', 'AUU', 'GUG', 'UCU', 'GUC', 'VAL', 'GUG', 'GLY', 'CUU', 'AUU', 'GGA', 'UUU', 'UAC', 'UCC', 'CUG', 'CCA', 'GGU', 'GAC', 'CCA', 'UAC', 'ASN', 'UGU', 'UAU', 'SER', 'AUU', 'UUC', 'UUA', 'ACU', 'GAU', 'GAU', 'GAA', 'AUU', 'AGG', 'UUG', 'GCG', 'AGG', 'AAA', 'AGA', 'UUA', 'AAA', 'GAA', 'AAC', 'CAA', 'ACA', 'GGU', 'AAA', 'AGU', 'GAU', 'UUU', 'GAA', 'ACA', 'AAA', 'GUA', 'UUC', 'GAU', 'AUU', 'LYS', 'CUG', 'TRP', 'AAA', 'ACA', 'AUU', 'PHE', 'AGU', 'GAU', 'UGG', 'AAA', 'AUA', 'UAC', 'AUU', 'UUA', 'ACU', 'UUA', 'UGG', 'AAU', 'AUU', 'PHE', 'UGU', 'UGG', 'AAU', 'GAC', 'AGU', 'AAU', 'VAL', 'UCA', 'UCU', 'GLY', 'GCA', 'UAC', 'CUA', 'CUA', 'TRP', 'UUG', 'LYS', 'UCU', 'UUG', 'AAA', 'AGA', 'UAC', 'UCU', 'ILE', 'PRO', 'AAG', 'CUC', 'AAU', 'CAG', 'UUA', 'UCC', 'AUG', 'ILE', 'THR', 'CCG', 'GGU', 'UUA', 'GGU', 'AUG', 'VAL', 'UAU', 'UUG', 'AUG', 'CUU', 'ACU', 'GGU', 'AUU', 'AUU', 'GCA', 'GAU', 'AAA', 'UUA', 'CAC', 'UCU', 'CGU', 'UGG', 'PHE', 'ALA', 'AUU', 'AUU', 'UUU', 'ACU', 'CAG', 'VAL', 'UUC', 'AAU', 'AUC', 'AUU', 'GGU', 'AAC', 'UCC', 'AUA', 'UUA', 'GCC', 'GCU', 'UGG', 'GAC', 'GUC', 'GCA', 'GAA', 'GGA', 'GCC', 'AAA', 'UGG', 'UUU', 'GCA', 'PHE', 'AUG', 'CUG', 'GLN', 'UGU', 'PHE', 'GGU', 'UGG', 'GCU', 'AUG', 'GCU', 'CCU', 'GUU', 'UUA', 'UAC', 'UCU', 'UGG', 'CAA', 'AAC', 'ASP', 'AUU', 'UGU', 'CGC', 'CGA', 'GAU', 'GCU', 'CAA', 'ACU', 'AGA', 'GCU', 'AUU', 'ACU', 'UUA', 'GUU', 'ACA', 'MET', 'AAU', 'AUU', 'AUG', 'GCU', 'CAA', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACG', '[PAD]', 'ACA', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGU', '[PAD]', '[PAD]', '[PAD]', 'GAA', 'CAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', 'AGG', '[PAD]', 'UGG', '[PAD]', '[PAD]', '[PAD]', 'UUC', '[PAD]', 'GAA', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGU', 'UGG', '[PAD]', 'AAA', 'CAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', 'CUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', 'AGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGA', '[PAD]', '[PAD]', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUA', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', 'UGG', '[PAD]', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', '[PAD]', '[PAD]', 'GGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGG', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', 'CCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', 'ACU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', 'GCG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', '[PAD]', '[PAD]', 'CAA', '[PAD]', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: -2.1731410026550293\n",
      "\n",
      "Example 2:\n",
      "Input Tokens: ['TE1', 'AUG', 'UCU', 'CUG', 'GCA', 'CAU', 'UAU', 'CYS', 'UUA', 'CUA', 'CUA', 'GCC', 'AUC', 'VAL', 'ACA', 'UUG', 'CUG', 'GGA', 'LEU', 'ACU', 'AAU', 'GUU', 'GUC', 'UCU', 'GCG', 'ACU', 'ACA', 'GCG', 'ALA', 'UGC', 'CUG', 'CCA', 'GCA', 'AAC', 'UCA', 'AGG', 'AAG', 'AAU', 'GGU', 'AUG', 'AAU', 'GUA', 'AAC', 'UUU', 'UAC', 'CAG', 'UAU', 'UCA', 'UUG', 'AGA', 'GAU', 'UCC', 'UCC', 'ACA', 'UAU', 'UCG', 'AAU', 'GCA', 'ALA', 'UAU', 'AUG', 'GCU', 'UAU', 'GGA', 'UAU', 'GCC', 'UCA', 'AAA', 'ACU', 'AAA', 'CUG', 'GLY', 'UCU', 'GUC', 'GGA', 'GGA', 'CAA', 'ACU', 'ASP', 'AUC', 'UCG', 'AUU', 'GAU', 'UAU', 'AAU', 'AUU', 'CCU', 'UGU', 'VAL', 'SER', 'UCA', 'UCA', 'GGC', 'ACA', 'PHE', 'CCU', 'UGU', 'CCU', 'CAA', 'GAA', 'GAU', 'UUA', 'UAU', 'GGU', 'ASN', 'UGG', 'GGA', 'UGC', 'AAA', 'GGA', 'AUU', 'GLY', 'ALA', 'UGU', 'UCU', 'AAU', 'AAU', 'CCA', 'ILE', 'AUU', 'GCA', 'UAC', 'UGG', 'AGU', 'ACU', 'GAU', 'UUA', 'UUU', 'GGU', 'UUC', 'UAU', 'ACU', 'ACC', 'CCA', 'ACA', 'AAC', 'GUA', 'THR', 'CUA', 'GAA', 'AUG', 'ACA', 'GGU', 'UAU', 'UUU', 'UUA', 'CCA', 'CCA', 'CAG', 'ACG', 'GLY', 'UCU', 'UAC', 'THR', 'UUC', 'AAG', 'UUU', 'GCU', 'ACA', 'GUU', 'GAC', 'GAC', 'UCU', 'GCA', 'AUU', 'CUA', 'UCA', 'GUC', 'GGU', 'GLY', 'AGC', 'AUU', 'ALA', 'UUC', 'GAA', 'UGU', 'CYS', 'GCA', 'CAA', 'GAA', 'CAA', 'CCU', 'CCC', 'AUC', 'THR', 'UCG', 'ACU', 'AAC', 'UUC', 'ACC', 'AUC', 'AAU', 'GGU', 'AUC', 'AAG', 'CCA', 'UGG', 'AAU', 'GGA', 'AGU', 'CCC', 'CCU', 'GAU', 'AAU', 'AUU', 'ACA', 'GLY', 'ACU', 'GUC', 'UAC', 'AUG', 'TYR', 'GCU', 'GGU', 'PHE', 'TYR', 'UAU', 'CCA', 'AUG', 'AAG', 'AUU', 'GUU', 'UAC', 'UCA', 'AAU', 'GCC', 'GUU', 'GCC', 'UGG', 'GGU', 'ACA', 'CUU', 'CCA', 'AUU', 'AGU', 'GUG', 'ACA', 'CUA', 'CCA', 'GAU', 'GGC', 'ACU', 'ACC', 'GUU', 'AGU', 'GAU', 'GAC', 'UUU', 'GAA', 'GGG', 'TYR', 'GUA', 'TYR', 'ACU', 'UUU', 'ASP', 'AAC', 'AAU', 'CUA', 'AGC', 'CAG', 'CCA', 'AAC', 'UGU', 'ACC', 'AUU', 'CCA', 'GAC', 'CCU', 'UCA', 'AAU', 'UAU', 'THR', 'GUC', 'AGU', 'THR', 'ACC', 'AUA', 'ACU', 'ACA', 'ACG', 'GAA', 'CCA', 'UGG', 'ACC', 'GGU', 'ACU', 'UUC', 'ACU', 'UCU', 'ACA', 'UCU', 'ACU', 'GAA', 'AUG', 'ACC', 'ACC', 'VAL', 'ACC', 'GGU', 'THR', 'AAC', 'GGC', 'GUU', 'CCA', 'ACU', 'GAC', 'GAA', 'ACC', 'GUC', 'AUU', 'GUC', 'AUC', 'AGA', 'ACU', 'CCA', 'ACA', 'ACU', 'GCU', 'AGC', 'ACC', 'AUC', 'AUA', 'ACU', 'THR', 'ACU', 'GAG', 'CCA', 'UGG', 'AAC', 'AGC', 'ACU', 'UUU', 'THR', 'UCU', 'ACU', 'UCU', 'ACC', 'GLU', 'UUG', 'ACC', 'ACA', 'GUC', 'ACU', 'GLY', 'ACC', 'AAU', 'GGU', 'GUA', 'CGA', 'ACU', 'GAC', 'GAA', 'ACC', 'AUC', 'AUU', 'GUA', 'AUC', 'AGA', 'ACA', 'CCA', 'ACA', 'ACA', 'GCC', 'ACU', 'ACU', 'GCC', 'AUA', 'ACU', 'ACA', 'ACU', 'GAG', 'CCA', 'UGG', 'AAC', 'AGC', 'THR', 'UUU', 'ACC', 'UCU', 'ACU', 'UCU', 'ACC', 'GAA', 'UUG', 'THR', 'ACA', 'GUC', 'ACC', 'GGU', 'ACC', 'AAU', 'GLY', 'UUG', 'CCA', 'ACU', 'GAU', 'GAG', 'ACC', 'AUC', 'ILE', 'GUC', 'AUC', 'AGA', 'ACA', 'CCA', 'ACA', 'ACA', 'GCC', 'ACU', 'ACU', 'GCC', 'AUG', 'ACU', 'ACA', 'ACU', 'CAG', 'PRO', 'UGG', 'AAC', 'GAC', 'ACU', 'UUU', 'ACC', 'UCU', 'ACU', 'UCU', 'ACC', 'GAA', 'UUG', 'ACC', 'ACA', 'GUC', 'THR', 'GGU', 'ACC', 'AAU', 'GGU', 'LEU', 'CCA', 'ACU', 'GAU', 'GAG', 'THR', 'AUC', 'AUU', 'GUC', 'AUC', 'AGA', 'ACA', 'CCA', 'ACA', 'ACA', 'GCC', 'ACU', 'ACU', 'GCC', 'AUG', 'THR', 'ACA', 'ACU', 'GLN', 'CCA', 'UGG', 'AAC', 'GAC', 'ACU', 'PHE', 'ACC', 'UCU', 'ACU', 'UCU', 'THR', 'GAA', 'UUG', 'ACC', 'ACA', 'GUC', 'ACC', 'GGU', 'ACC', 'AAU', 'GGU', 'UUG', 'CCA', 'ACU', 'GAU', 'GAG', 'ACC', 'AUC', 'AUU', 'GUC', 'AUC', 'AGA', 'ACA', 'CCA', 'ACA', 'ACA', 'GCC', 'ACU', 'THR', 'GCC', 'AUG', 'THR', 'ACA', 'ACU', 'CAG', 'CCA', 'UGG', 'AAC', 'GAC', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', 'AGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', 'GCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', 'ACA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', 'GCG', '[PAD]', '[PAD]', '[PAD]', 'UGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAU', '[PAD]', '[PAD]', 'UUC', 'UAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAC', '[PAD]', 'UAU', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', 'CAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: -3.248647928237915\n",
      "\n",
      "Example 3:\n",
      "Input Tokens: ['TE1', 'AUG', 'UCG', 'CCC', 'UCU', 'GCC', 'GUA', 'CAA', 'UCA', 'UCA', 'AAA', 'CUA', 'GAA', 'GAA', 'CAG', 'UCA', 'AGU', 'GAA', 'AUU', 'GAC', 'AAG', 'LEU', 'AAA', 'GCA', 'AAA', 'AUG', 'SER', 'CAG', 'SER', 'GCC', 'ALA', 'ACU', 'GCG', 'CAG', 'CAG', 'LYS', 'AAG', 'GAA', 'CAU', 'GAG', 'UAU', 'GAA', 'CAU', 'UUG', 'ACU', 'UCG', 'VAL', 'AAG', 'AUC', 'GUG', 'PRO', 'CAA', 'CGG', 'CCC', 'AUC', 'SER', 'GAU', 'AGA', 'LEU', 'CAG', 'CCC', 'GCA', 'AUU', 'ALA', 'ACC', 'CAC', 'UAU', 'UCU', 'CCA', 'CAC', 'LEU', 'GAC', 'GGG', 'UUG', 'CAG', 'GAC', 'UAU', 'CAG', 'CGC', 'UUG', 'CAC', 'AAG', 'GAG', 'UCU', 'ILE', 'GAA', 'GAC', 'PRO', 'ALA', 'AAG', 'UUC', 'UUC', 'GGU', 'UCU', 'AAA', 'ALA', 'THR', 'CAA', 'UUU', 'UUA', 'AAC', 'UGG', 'SER', 'AAG', 'CCA', 'UUC', 'GAU', 'AAG', 'GUG', 'UUC', 'AUC', 'CCA', 'GAC', 'CCU', 'AAA', 'ACG', 'GGC', 'AGG', 'CCC', 'UCC', 'UUC', 'CAG', 'AAC', 'AAU', 'GCA', 'UGG', 'UUC', 'CUC', 'ASN', 'GGC', 'GLN', 'UUA', 'AAC', 'GCC', 'UGU', 'UAC', 'AAC', 'UGU', 'GUU', 'GAC', 'AGA', 'CAU', 'GCC', 'LEU', 'LYS', 'THR', 'CCU', 'AAC', 'AAG', 'AAA', 'GCC', 'AUU', 'AUU', 'UUC', 'GAA', 'GGU', 'GAC', 'GAG', 'CCU', 'GGC', 'CAA', 'GGC', 'UAU', 'UCC', 'AUU', 'ACC', 'UAC', 'AAG', 'GAA', 'CUA', 'CUU', 'GAA', 'GAA', 'GUU', 'UGU', 'GLN', 'GUG', 'GCA', 'CAA', 'GUG', 'CUG', 'ACU', 'UAC', 'UCU', 'AUG', 'GGC', 'GUU', 'CGC', 'AAG', 'GGC', 'GAU', 'ACU', 'VAL', 'ALA', 'GUG', 'UAC', 'AUG', 'CCU', 'AUG', 'GUC', 'CCA', 'GAA', 'GCA', 'AUC', 'AUA', 'ACC', 'UUG', 'UUG', 'GCC', 'ILE', 'UCC', 'CGU', 'AUC', 'GGU', 'GCC', 'AUU', 'CAC', 'UCC', 'GUA', 'GUC', 'PHE', 'GCC', 'GLY', 'UUU', 'UCU', 'UCC', 'AAC', 'UCC', 'UUG', 'AGA', 'GAU', 'CGU', 'AUC', 'AAC', 'GAU', 'GGG', 'GAC', 'UCU', 'LYS', 'GUU', 'GUC', 'AUC', 'ACU', 'ACA', 'GAU', 'GAA', 'UCC', 'AAC', 'AGA', 'GGU', 'GGU', 'AAA', 'VAL', 'AUU', 'GAG', 'ACU', 'AAA', 'AGA', 'AUU', 'GUU', 'GAU', 'GAC', 'GCG', 'CUA', 'AGA', 'GAG', 'ACC', 'CCA', 'GGC', 'GUG', 'AGA', 'CAC', 'GUC', 'UUG', 'GUU', 'TYR', 'AGA', 'LYS', 'ACC', 'AAC', 'AAU', 'CCA', 'SER', 'GUU', 'GCU', 'UUC', 'CAU', 'ALA', 'CCC', 'AGA', 'GAU', 'UUG', 'GAU', 'UGG', 'GCA', 'ACA', 'GAA', 'AAG', 'AAG', 'LYS', 'UAC', 'AAG', 'ACC', 'UAC', 'UAU', 'CCA', 'UGC', 'ACA', 'CCC', 'GUU', 'GAU', 'UCU', 'GAG', 'GAU', 'CCA', 'UUA', 'UUC', 'UUG', 'UUG', 'UAU', 'ACG', 'UCU', 'GGU', 'UCU', 'ACU', 'GGU', 'GCC', 'CCC', 'AAG', 'GGU', 'GUU', 'CAA', 'CAU', 'UCU', 'THR', 'GCA', 'GGU', 'UAC', 'UUG', 'CUG', 'GGA', 'GCU', 'UUG', 'UUG', 'ACC', 'MET', 'CGC', 'UAC', 'ACU', 'UUU', 'GAC', 'ACU', 'HIS', 'CAA', 'GAA', 'GAC', 'GUU', 'PHE', 'UUC', 'ACA', 'GCU', 'GGA', 'GAC', 'AUU', 'GGC', 'UGG', 'AUU', 'ACA', 'GGC', 'CAC', 'ACU', 'UAU', 'GUG', 'GUU', 'UAU', 'GLY', 'CCC', 'UUA', 'CUA', 'UAU', 'GGU', 'UGU', 'GCC', 'ACU', 'UUG', 'GUC', 'UUU', 'GAA', 'GGG', 'ACU', 'CCU', 'GCG', 'UAC', 'CCA', 'AAU', 'UAC', 'UCC', 'CGU', 'UAU', 'UGG', 'GAU', 'AUU', 'AUU', 'GAU', 'GAA', 'HIS', 'AAA', 'GUC', 'ACC', 'CAA', 'UUU', 'UAU', 'GUU', 'GCG', 'CCA', 'ACU', 'GCU', 'UUG', 'CGU', 'UUG', 'UUG', 'AAA', 'AGA', 'GCU', 'GGU', 'GAU', 'UCC', 'TYR', 'AUC', 'GLU', 'AAU', 'CAU', 'UCC', 'UUA', 'AAA', 'SER', 'UUG', 'CGU', 'UGC', 'UUG', 'GGU', 'UCG', 'GUC', 'GGU', 'GAG', 'CCA', 'AUU', 'GCU', 'GCU', 'GAA', 'GUU', 'UGG', 'GAG', 'UGG', 'UAC', 'UCU', 'GAA', 'AAA', 'AUA', 'GGU', 'AAA', 'AAU', 'GLU', 'AUC', 'CCC', 'AUU', 'GUA', 'GAC', 'ACC', 'UAC', 'UGG', 'CAA', 'ACA', 'GAA', 'UCU', 'GGU', 'SER', 'CAU', 'LEU', 'GUC', 'ACC', 'CCG', 'CUG', 'GCU', 'GGU', 'GGU', 'VAL', 'ACA', 'CCA', 'AUG', 'AAA', 'CCG', 'GGU', 'UCU', 'GCC', 'UCA', 'UUC', 'CCC', 'UUC', 'UUC', 'GGU', 'AUU', 'GAU', 'GCA', 'GUU', 'GUU', 'CUU', 'GAC', 'PRO', 'AAC', 'ACU', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', 'UCU', '[PAD]', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', '[PAD]', 'CCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCA', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', 'CCU', 'GCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCU', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', 'AAG', 'ACU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', '[PAD]', 'GGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAU', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAC', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCG', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCU', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: -2.935976982116699\n",
      "\n",
      "Example 4:\n",
      "Input Tokens: ['TE8', 'AUG', 'GCA', 'UCU', 'AAU', 'CAA', 'CCU', 'GGC', 'AAG', 'CYS', 'CYS', 'UUU', 'GAA', 'GGA', 'GUU', 'UGU', 'CAC', 'GAU', 'GGA', 'ACA', 'CCC', 'AAG', 'GGU', 'CGU', 'CGU', 'GAA', 'GAA', 'AUC', 'UUC', 'GGU', 'UUA', 'GAU', 'THR', 'UAU', 'GCA', 'GCA', 'GLY', 'UCU', 'ACA', 'UCU', 'CCC', 'AAG', 'GAA', 'AAA', 'GUU', 'AUA', 'GUU', 'AUC', 'UUG', 'ACA', 'GAU', 'GUG', 'UAU', 'GGC', 'AAU', 'AAA', 'UUC', 'AAC', 'AAU', 'GUU', 'UUA', 'UUA', 'ACG', 'GCC', 'ASP', 'AAA', 'UUU', 'GCU', 'AGU', 'GCU', 'GGG', 'UAC', 'AUG', 'GUC', 'UUU', 'GUU', 'CCC', 'ASP', 'AUU', 'UUA', 'PHE', 'GGC', 'GAU', 'GCU', 'AUC', 'UCA', 'UCG', 'GAC', 'AAA', 'CCA', 'AUU', 'GAU', 'CGU', 'ASP', 'GCC', 'TRP', 'UUU', 'GLN', 'AGA', 'CAU', 'SER', 'CCU', 'GAA', 'GUC', 'THR', 'AAG', 'AAA', 'AUU', 'GUU', 'GAU', 'GGA', 'UUC', 'AUG', 'AAG', 'UUG', 'UUA', 'AAA', 'CUU', 'GAA', 'UAU', 'GAC', 'PRO', 'AAG', 'UUU', 'AUU', 'GGC', 'GUU', 'GUG', 'GGU', 'UAC', 'UGU', 'UUU', 'GGU', 'GCA', 'AAG', 'UUU', 'ALA', 'GUC', 'CAA', 'CAC', 'AUU', 'AGU', 'GGC', 'GAC', 'GGG', 'GGU', 'CUU', 'ALA', 'AAU', 'GCU', 'GCA', 'GCC', 'AUU', 'GCA', 'CAU', 'CCA', 'UCU', 'UUC', 'GUC', 'AGC', 'AUC', 'GAG', 'GAA', 'ILE', 'GAA', 'GCA', 'AUU', 'GAU', 'AGC', 'AAG', 'AAA', 'CCA', 'AUA', 'UUG', 'AUU', 'UCA', 'GCA', 'GCG', 'GAA', 'GAG', 'GAU', 'CAC', 'AUC', 'UUU', 'CCG', 'ALA', 'AAC', 'UUA', 'AGA', 'CAC', 'UUA', 'ACG', 'GAG', 'GAA', 'AAA', 'UUA', 'LYS', 'GAU', 'AAU', 'CAC', 'ALA', 'ACU', 'UAC', 'CAG', 'UUA', 'GAC', 'CUC', 'UUC', 'AGU', 'GGU', 'GUG', 'GCU', 'CAC', 'GGG', 'UUU', 'GCA', 'GCA', 'AGA', 'GGC', 'GAU', 'AUA', 'SER', 'AUA', 'CCU', 'GCC', 'GUA', 'AAA', 'UAU', 'GCG', 'AAG', 'GAG', 'AAA', 'GUC', 'UUG', 'CUC', 'GAC', 'CAA', 'AUA', 'UAC', 'TRP', 'UUC', 'AAU', 'CAU', 'UUU', 'UCG', 'AAU', 'GUU', 'UAA', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGU', 'UGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', 'UGG', '[PAD]', 'CAA', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', 'GCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: 0.7279980182647705\n",
      "\n",
      "Example 5:\n",
      "Input Tokens: ['TE5', 'AUG', 'GUA', 'CGU', 'CGA', 'UGG', 'AUU', 'CCU', 'AGU', 'GGC', 'AGG', 'HIS', 'LEU', 'CGC', 'AAU', 'ASN', 'ASP', 'AAC', 'ACU', 'GGU', 'ASP', 'GAU', 'GAC', 'GAC', 'AGC', 'GAG', 'UUC', 'ACA', 'AAC', 'UCG', 'AUG', 'ASP', 'SER', 'GLY', 'AUG', 'UCC', 'AUA', 'CCA', 'UCA', 'LEU', 'AGG', 'GAC', 'SER', 'AUG', 'ACC', 'ACG', 'ARG', 'UCA', 'UCU', 'CAU', 'AAC', 'GAU', 'CCC', 'AUC', 'LYS', 'CCU', 'GCU', 'CUG', 'AUG', 'AAC', 'GAU', 'UCC', 'AAC', 'AAA', 'GUC', 'AAA', 'AAU', 'UUG', 'GAG', 'AAG', 'GAG', 'UUG', 'THR', 'AAU', 'GCC', 'LYS', 'AUC', 'AAG', 'AUU', 'CAA', 'GUA', 'LEU', 'UAU', 'GAA', 'UAC', 'AUU', 'CGC', 'AGA', 'AUC', 'CCU', 'AAU', 'AAA', 'GAC', 'GGC', 'AAU', 'GCA', 'CCA', 'UCG', 'LEU', 'GGC', 'AAU', 'GAC', 'ACU', 'ASP', 'PHE', 'AGA', 'AAU', 'UCG', 'AUU', 'AUC', 'GAG', 'GGU', 'CUA', 'AAU', 'CUU', 'GAA', 'AUA', 'AAC', 'AAA', 'UUG', 'AAA', 'CAG', 'GAU', 'UUA', 'AAG', 'GCG', 'AAG', 'GAA', 'GUC', 'GLU', 'UAC', 'CAA', 'GAU', 'ACG', 'CUA', 'CAA', 'UUC', 'GUU', 'CAA', 'GAG', 'AAU', 'UUA', 'GAA', 'AAU', 'UCG', 'GAA', 'AGU', 'AUC', 'GUG', 'AAU', 'ACG', 'AUC', 'AAU', 'CAC', 'CUA', 'LEU', 'UCC', 'UUU', 'AUA', 'UUG', 'ACG', 'HIS', 'UUU', 'AAU', 'GAG', 'CAA', 'ASP', 'GAA', 'AAU', 'GCC', 'HIS', 'CUU', 'LEU', 'GAU', 'AAA', 'GAA', 'GAG', 'AGG', 'GAG', 'ACC', 'LEU', 'GAG', 'GAA', 'ACU', 'UUA', 'GLU', 'LEU', 'AGC', 'UCG', 'ASP', 'UAU', 'GUU', 'CUU', 'GAA', 'AAA', 'AUG', 'GAU', 'ACU', 'UUG', 'UCC', 'AAG', 'UUC', 'AUC', 'ILE', 'CAA', 'UUC', 'UUG', 'CAG', 'GAC', 'PHE', 'UUG', 'CAU', 'UCC', 'AAA', 'AGU', 'CGA', 'GCG', 'GLU', 'UCA', 'AAG', 'CAG', 'ASP', 'AAG', 'GAA', 'GAA', 'UUU', 'CUU', 'UCA', 'LEU', 'GCC', 'CAG', 'UCC', 'UCA', 'CCA', 'GCA', 'GGA', 'UCA', 'CAG', 'UUA', 'GAA', 'AGU', 'AGG', 'GAC', 'UCA', 'CCA', 'UCA', 'AGU', 'AAA', 'GLU', 'GAG', 'AAU', 'ACU', 'ASP', 'GGU', 'GGA', 'UAC', 'CAG', 'AAU', 'GAC', 'GAA', 'ILE', 'CAU', 'GAC', 'AGU', 'AAC', 'AAC', 'CAC', 'AUU', 'GAU', 'ACA', 'GAA', 'AAU', 'VAL', 'AUG', 'GCA', 'AAC', 'AGU', 'ACU', 'UCC', 'LEU', 'CCA', 'AUU', 'UCA', 'GCC', 'VAL', 'GAG', 'UCU', 'CGU', 'UUU', 'GAG', 'AAA', 'THR', 'UUA', 'GAC', 'ACU', 'GLN', 'UUA', 'GAG', 'AUU', 'GUC', 'AUA', 'GLU', 'AAU', 'UUG', 'CAC', 'AAA', 'GAA', 'UAU', 'GAU', 'CAA', 'UUU', 'AUA', 'AAU', 'UCC', 'AUU', 'AGA', 'UUG', 'AAA', 'UUC', 'GAG', 'AAA', 'UCG', 'CAA', 'AAA', 'UUA', 'GAA', 'AAA', 'AUA', 'AUA', 'GCG', 'UCC', 'AAA', 'CUA', 'AAU', 'GAG', 'CAG', 'UCU', 'CAU', 'CUA', 'CUA', 'GAU', 'UCC', 'UUA', 'GAA', 'LEU', 'GAA', 'GAA', 'AAU', 'SER', 'AGU', 'UCC', 'GUU', 'AUA', 'GAA', 'AAA', 'CAA', 'GAU', 'CAU', 'UUG', 'AUU', 'UCC', 'GLN', 'CUG', 'AAG', 'GAA', 'AAG', 'AUC', 'GAA', 'UCG', 'CAA', 'UCU', 'GUC', 'LEU', 'AUA', 'AAC', 'ASN', 'UUG', 'GAA', 'AAA', 'LEU', 'AAG', 'GAA', 'GAC', 'AUC', 'AUU', 'AAA', 'AUG', 'AAA', 'CAA', 'AAU', 'GAA', 'AAA', 'GUU', 'UUA', 'ACC', 'AAA', 'GAA', 'CUG', 'GAA', 'ACU', 'CAG', 'ACC', 'AAA', 'AUC', 'AAU', 'AAA', 'CUA', 'AAG', 'GAA', 'AAU', 'AAU', 'UGG', 'GAC', 'AGC', 'UAC', 'AUC', 'AAU', 'GAC', 'UUG', 'GAA', 'AAA', 'CAA', 'AUC', 'AAU', 'GAC', 'CUU', 'CAA', 'AUC', 'GAU', 'AAA', 'UCA', 'GAG', 'GAA', 'UUC', 'CAC', 'GUA', 'AUA', 'CAA', 'AAU', 'CAG', 'UUA', 'GAC', 'AAA', 'LEU', 'GAC', 'LEU', 'GAG', 'AAU', 'TYR', 'GLN', 'LEU', 'AAA', 'ASN', 'CAG', 'UUA', 'AAU', 'ACU', 'UUG', 'GAC', 'AAC', 'CAA', 'LYS', 'UUA', 'AUA', 'CUA', 'UCC', 'CAA', 'UAU', 'GAA', 'AGC', 'AAU', 'UUU', 'AUC', 'AAA', 'UUU', 'AAU', 'CAG', 'ASN', 'CUU', 'UUA', 'CUA', 'CAC', 'LEU', 'GAC', 'AGU', 'AUU', 'UUC', 'AAU', 'AUU', 'CUG', 'CAG', 'AAA', 'ILE', 'UUA', 'CAA', 'GAA', 'AGU', 'UCU', 'AUU', 'GCC', 'CAA', 'UUU', 'GAC', 'AGG', 'AAA', 'AUG', 'AAA', 'UCC', 'AUA', 'AAA', 'UCU', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAU', 'CUU', '[PAD]', '[PAD]', 'AAU', 'GAC', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', 'UCU', 'GGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUC', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', 'AGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', 'CAU', '[PAD]', 'CUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAG', 'CUG', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAG', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUA', '[PAD]', 'UUG', '[PAD]', '[PAD]', 'UAC', 'CAA', 'UUA', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: -0.1595979928970337\n",
      "\n",
      "Example 6:\n",
      "Input Tokens: ['TE6', 'AUG', 'AAG', 'CUC', 'CCA', 'GLN', 'ACC', 'AUG', 'CUA', 'CGU', 'UCU', 'AUA', 'UCU', 'GUG', 'AAG', 'HIS', 'GUC', 'CGG', 'UGG', 'PRO', 'AGG', 'AUU', 'CUG', 'ACG', 'GLY', 'UCA', 'AAG', 'CUU', 'UGG', 'UAC', 'UCA', 'ACG', 'CAG', 'AUG', 'GCA', 'AUG', 'ACU', 'CCG', 'GAG', 'GAG', 'AAG', 'AUG', 'AUC', 'ACC', 'GAU', 'AAA', 'CUA', 'CAA', 'CAG', 'GAA', 'CUG', 'GAA', 'CCU', 'GAA', 'GUG', 'UGU', 'AAA', 'GUG', 'CAA', 'ASP', 'GUU', 'UCC', 'GGU', 'GGC', 'UGC', 'GGA', 'UCC', 'AUG', 'UUU', 'GCU', 'ILE', 'ASN', 'AUA', 'ACA', 'AGC', 'AAG', 'AAG', 'UUC', 'AAC', 'GLY', 'CUG', 'AGU', 'CUC', 'AUC', 'AAG', 'CAG', 'CAC', 'CAG', 'CUG', 'GUG', 'AAC', 'AGA', 'AUU', 'UUG', 'ARG', 'ASP', 'ASP', 'AUU', 'UCC', 'AGA', 'UGG', 'CAU', 'GGC', 'CUA', 'CAA', 'UUG', 'ACC', 'ACU', 'AAG', 'AAG', 'UCA', 'ACU', 'GGG', 'AAG', 'GGU', 'CCG', 'GCA', 'UCA', 'UCA', 'UGA', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAU', '[PAD]', '[PAD]', '[PAD]', 'CCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGG', 'GAC', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: 0.33780598640441895\n",
      "\n",
      "Example 7:\n",
      "Input Tokens: ['TE10', 'AUG', 'UUA', 'CGC', 'ACU', 'ACU', 'AGA', 'CUA', 'UGG', 'ACC', 'THR', 'ARG', 'AUG', 'CCC', 'ACU', 'GUG', 'AGC', 'AAA', 'UUG', 'UUU', 'UUG', 'AGA', 'AAC', 'AGC', 'UCC', 'GGC', 'AAU', 'GCC', 'CUA', 'AAC', 'AAG', 'AAU', 'AAA', 'CUA', 'CCA', 'UUU', 'LEU', 'UAC', 'UCA', 'UCC', 'CAA', 'GGA', 'CCU', 'CAA', 'GCC', 'GUG', 'AGG', 'UAC', 'ACU', 'UCC', 'CAA', 'CAU', 'GAG', 'UGG', 'AUA', 'GCU', 'GUG', 'CAU', 'CAG', 'GAC', 'AAG', 'ACU', 'GCC', 'UUU', 'GUC', 'GGA', 'AUU', 'ACA', 'LYS', 'UAC', 'GCC', 'ACU', 'GAU', 'GCC', 'UUA', 'GGG', 'GAC', 'GCU', 'ACC', 'UAU', 'GUU', 'GAG', 'UUG', 'CCA', 'GAA', 'GUG', 'GGC', 'ACU', 'GAG', 'AUU', 'GCC', 'CAA', 'GGU', 'GAG', 'UCG', 'LEU', 'GGG', 'UCC', 'AUU', 'GAG', 'UCC', 'GUC', 'AAG', 'UCA', 'GCC', 'UCC', 'GAG', 'AUC', 'UAC', 'CAG', 'CCU', 'GCC', 'GAU', 'GGU', 'ACC', 'GUA', 'GAG', 'GAA', 'AUU', 'AAC', 'ACU', 'AAU', 'LEU', 'GAG', 'GAA', 'AAU', 'CCA', 'GGU', 'GUG', 'GUG', 'AAC', 'GAA', 'GAU', 'CCU', 'AUG', 'GGU', 'GAC', 'GLY', 'UGG', 'CUA', 'GUC', 'AAA', 'AUG', 'AAG', 'CUU', 'GGU', 'GAG', 'GGC', 'GUU', 'AAU', 'GUG', 'GAA', 'GLN', 'GUC', 'GAG', 'GLY', 'CUA', 'AUG', 'UCC', 'UUA', 'GAA', 'CAG', 'UAC', 'GAA', 'AAG', 'ACA', 'CUG', 'GUU', 'CAU', 'GAU', 'ASP', 'UGA', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', 'CGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAG', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: 1.498589038848877\n",
      "\n",
      "Example 8:\n",
      "Input Tokens: ['TE6', 'AUG', 'UCA', 'UCU', 'GCA', 'GAG', 'AUG', 'GAA', 'CAA', 'UUG', 'UUA', 'CAG', 'GCC', 'AAG', 'ACA', 'CUG', 'GCC', 'MET', 'CAC', 'AAC', 'AAU', 'CCA', 'ACG', 'GLU', 'AUG', 'CUG', 'PRO', 'AAG', 'GUG', 'CUC', 'GAA', 'ACU', 'ACG', 'GCA', 'UCC', 'AUG', 'UAC', 'CAC', 'AAC', 'GLY', 'AAU', 'LEU', 'AGC', 'AAG', 'CUG', 'AAG', 'UUG', 'CCU', 'LEU', 'GCC', 'AAG', 'UUU', 'UUU', 'ACA', 'CAG', 'UUA', 'GUU', 'CUA', 'ASP', 'GUG', 'GUG', 'UCG', 'AUG', 'GAC', 'UCU', 'CCA', 'AUU', 'GCG', 'AAU', 'ACU', 'GAG', 'AGA', 'CCG', 'UUU', 'AUU', 'GCU', 'GCU', 'CAA', 'UAU', 'CUG', 'CCA', 'CUA', 'CUU', 'CUU', 'GCU', 'AUG', 'GCG', 'CAA', 'UCC', 'ACC', 'ALA', 'ASP', 'GUA', 'CUA', 'GUG', 'UAC', 'AAG', 'AAU', 'AUC', 'GUG', 'CUU', 'ILE', 'AUG', 'UGC', 'GCU', 'UCA', 'TYR', 'CCG', 'CUG', 'GUG', 'UUG', 'ASP', 'CUG', 'GUU', 'GCU', 'LYS', 'ACA', 'UCA', 'AAC', 'CAG', 'GAA', 'AUG', 'UUU', 'ASP', 'CAG', 'UUG', 'UGU', 'MET', 'CUG', 'AAG', 'AAG', 'UUC', 'GUG', 'CUC', 'UCG', 'CAC', 'UGG', 'AGA', 'ACU', 'ALA', 'UAU', 'CCU', 'UUG', 'CGU', 'GCC', 'ACC', 'VAL', 'GAC', 'GAU', 'GLU', 'ACG', 'GAU', 'GUC', 'GLU', 'CAA', 'UGG', 'CUG', 'GCG', 'CAG', 'AUU', 'GAC', 'CAA', 'AAU', 'AUC', 'GGC', 'GUG', 'AAA', 'UUA', 'GCG', 'ACC', 'AUC', 'AAG', 'UUC', 'AUA', 'UCU', 'GAG', 'GUC', 'GUG', 'CUG', 'UCG', 'GLN', 'ACU', 'AAA', 'UCA', 'CCC', 'AGC', 'GGC', 'AAC', 'GAG', 'AUU', 'AAU', 'UCA', 'UCU', 'ACC', 'AUC', 'CCG', 'GAU', 'AAC', 'CAC', 'CCU', 'GUG', 'UUG', 'AAC', 'AAA', 'CCG', 'GCU', 'UUG', 'GAG', 'AGC', 'GAG', 'GCU', 'AAG', 'ARG', 'CUU', 'CUU', 'GAU', 'AUG', 'UUG', 'CUA', 'AAC', 'UAC', 'CUA', 'AUU', 'GLU', 'GAA', 'CAG', 'UAC', 'AUG', 'GUC', 'SER', 'UCC', 'GUU', 'UUC', 'AUU', 'GGU', 'AUC', 'AUC', 'AAU', 'UCU', 'LEU', 'UCC', 'UUC', 'GUC', 'AUC', 'AAA', 'AGA', 'AGG', 'CCG', 'CAG', 'THR', 'ACA', 'AUA', 'AGA', 'AUU', 'CUU', 'SER', 'GGG', 'CUG', 'UUG', 'CGU', 'UUC', 'AAC', 'GUC', 'GAC', 'GCC', 'AAG', 'UUU', 'CCC', 'CUA', 'GAG', 'GGC', 'AAG', 'UCU', 'GAC', 'UUG', 'AAC', 'UAC', 'AAA', 'CUA', 'SER', 'AAG', 'AGA', 'UUU', 'GUU', 'GAA', 'AGG', 'GCG', 'UAC', 'AAG', 'AAC', 'UUU', 'GUG', 'GLN', 'UUU', 'GGG', 'CUA', 'AAA', 'AAU', 'CAA', 'ILE', 'AUU', 'ACA', 'AAA', 'UCC', 'CUC', 'UCA', 'UCC', 'GGA', 'UCA', 'GGG', 'UCA', 'UCG', 'AUC', 'UAC', 'UCC', 'AAG', 'CUG', 'ACC', 'AAG', 'AUU', 'UCU', 'CAA', 'ACU', 'UUA', 'CAC', 'GUU', 'AUU', 'GGC', 'GAA', 'GAG', 'THR', 'AAG', 'AGC', 'AAG', 'GGA', 'AUU', 'UUG', 'AAC', 'UUC', 'GAC', 'CCU', 'SER', 'AAG', 'GGC', 'ASN', 'AGC', 'AAG', 'LYS', 'ACG', 'UUG', 'UCC', 'AGG', 'CAG', 'GAC', 'AAA', 'CUA', 'AAA', 'UAC', 'AUC', 'UCA', 'CUA', 'UGG', 'AAA', 'AGG', 'GLN', 'UUA', 'UCC', 'ALA', 'UUA', 'UUG', 'UCU', 'ACU', 'CUA', 'GGG', 'GUG', 'UCC', 'ACA', 'AAG', 'ACC', 'CCC', 'ACG', 'CCU', 'GUG', 'UCC', 'GCA', 'CCU', 'GCA', 'ACG', 'GGC', 'UCU', 'UCA', 'ACC', 'GAA', 'AAC', 'AUG', 'CUU', 'GAU', 'CAA', 'CUG', 'AAG', 'ILE', 'UUG', 'CAA', 'AAA', 'UAC', 'THR', 'CUC', 'ASN', 'AAG', 'GCU', 'UCA', 'CAC', 'CAG', 'GGC', 'ASN', 'ACU', 'UUU', 'UUC', 'ASN', 'AAC', 'UCA', 'CCC', 'AAA', 'CCA', 'AUC', 'AGC', 'ASN', 'ACC', 'UAC', 'UCA', 'UCU', 'GUG', 'UAC', 'UCA', 'UUG', 'AUG', 'AAC', 'AGU', 'UCG', 'AAC', 'SER', 'AAC', 'CAG', 'GAU', 'GUG', 'ACC', 'CAG', 'CUA', 'PRO', 'AAU', 'GAC', 'AUA', 'CUU', 'AUC', 'AAG', 'CUG', 'UCC', 'ACA', 'GAG', 'GCC', 'ILE', 'UUG', 'CAA', 'AUG', 'GAC', 'AGC', 'ACG', 'AAA', 'CUG', 'AUC', 'ACC', 'GGA', 'UUG', 'UCU', 'ILE', 'GUU', 'GCU', 'UCG', 'AGG', 'UAC', 'ACG', 'GAU', 'UUA', 'AUG', 'ASN', 'ACG', 'TYR', 'ILE', 'AAU', 'UCU', 'GUA', 'CCG', 'UCC', 'UCG', 'UCA', 'UCA', 'UCA', 'AAG', 'AGG', 'AAA', 'UCC', 'GAC', 'GAU', 'GAU', 'GAC', 'GAC', 'GLY', 'AAC', 'GAC', 'AAU', 'GAA', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAG', '[PAD]', '[PAD]', 'CCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', 'CUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCG', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', 'GCG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', 'UAC', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: 0.08772800117731094\n",
      "\n",
      "Example 9:\n",
      "Input Tokens: ['TE2', 'AUG', 'GCC', 'AUA', 'UUG', 'AAG', 'GAU', 'ACC', 'ILE', 'AUU', 'AGA', 'UAC', 'GCU', 'AAU', 'GCA', 'AGG', 'UAU', 'GCU', 'ACC', 'ALA', 'SER', 'GGC', 'ACU', 'UCC', 'ACC', 'GCC', 'ACU', 'GCC', 'ALA', 'UCU', 'GUC', 'AGC', 'GCU', 'GCC', 'UCA', 'UGU', 'PRO', 'AAU', 'UUG', 'CCC', 'UUG', 'CUC', 'UUG', 'CAA', 'AAG', 'AGG', 'ARG', 'ALA', 'AUU', 'GCU', 'AGU', 'ALA', 'AAG', 'UCU', 'AAA', 'ASN', 'CCU', 'ASN', 'CUC', 'GUU', 'AAA', 'AGA', 'GAA', 'UUG', 'CAA', 'GCA', 'HIS', 'CAC', 'UCA', 'GCG', 'AUC', 'AGC', 'GAA', 'UAC', 'AAU', 'AAU', 'GAU', 'CAA', 'UUG', 'GAC', 'CAC', 'UAU', 'UUC', 'CGU', 'LEU', 'UCC', 'CAC', 'ACA', 'GAA', 'AGG', 'CCG', 'CUG', 'UAC', 'AAC', 'LEU', 'ACU', 'AAC', 'UUC', 'ASN', 'UCU', 'GLN', 'PRO', 'CAA', 'GUU', 'AAU', 'PRO', 'AAG', 'AUG', 'CGU', 'UUC', 'UUG', 'AUC', 'UUU', 'GAC', 'UUC', 'AUC', 'MET', 'UAC', 'UGU', 'CAC', 'ACA', 'AGA', 'CUC', 'AAU', 'CUA', 'UCG', 'ACC', 'UCG', 'ACU', 'UUG', 'UUC', 'CUU', 'ACU', 'UUC', 'ACU', 'AUC', 'UUG', 'GAC', 'AAG', 'UAU', 'SER', 'UCG', 'CGG', 'UUC', 'AUU', 'AUC', 'AAG', 'AGU', 'UAC', 'AAC', 'TYR', 'CAG', 'CUC', 'UUG', 'UCC', 'UUG', 'THR', 'GCG', 'CUU', 'UGG', 'AUU', 'UCG', 'UCC', 'LYS', 'UUU', 'UGG', 'GAC', 'UCC', 'LYS', 'AAU', 'ARG', 'AUG', 'GCC', 'THR', 'LEU', 'AAA', 'GUC', 'UUG', 'GLN', 'AAC', 'UUG', 'UGU', 'UGC', 'ASN', 'CAA', 'UAU', 'UCU', 'AUA', 'AAG', 'CAA', 'UUC', 'ACG', 'ACU', 'AUG', 'GAA', 'MET', 'CAU', 'CUU', 'PHE', 'AAA', 'UCA', 'CUC', 'GAU', 'TRP', 'UCC', 'AUC', 'UGU', 'CAG', 'UCG', 'GCA', 'ACA', 'UUC', 'GAC', 'UCC', 'UAC', 'AUC', 'GAC', 'AUC', 'UUC', 'UUG', 'UUC', 'CAA', 'UCU', 'ACG', 'SER', 'CCG', 'UUA', 'UCG', 'CCU', 'GLY', 'GUU', 'GUC', 'CUU', 'SER', 'GCC', 'CCU', 'UUG', 'GAA', 'GCU', 'UUC', 'AUU', 'CAA', 'CAG', 'AAA', 'CUG', 'GCC', 'UUA', 'UUA', 'AAU', 'AAC', 'GCU', 'GCU', 'GLY', 'ACU', 'GCU', 'AUU', 'AAU', 'AAA', 'UCG', 'UCC', 'UCU', 'UCU', 'CAA', 'GGC', 'CCC', 'UCU', 'UUG', 'AAC', 'AUC', 'AAC', 'GAG', 'ILE', 'LYS', 'UUG', 'GGU', 'GCC', 'ILE', 'AUG', 'UUG', 'UGC', 'GAG', 'UUA', 'ALA', 'UCC', 'UUC', 'AAU', 'CUC', 'GAA', 'UUA', 'UCA', 'UUU', 'AAA', 'UAU', 'GAU', 'ARG', 'UCA', 'CUA', 'AUU', 'GCG', 'CUG', 'GGU', 'GCA', 'AUU', 'AAC', 'CUC', 'AUC', 'AAA', 'UUA', 'UCU', 'UUG', 'AAC', 'UAC', 'UAU', 'AAU', 'UCA', 'AAC', 'CUU', 'UGG', 'GAA', 'AAU', 'ILE', 'AAU', 'CUG', 'GCU', 'LEU', 'GLU', 'GAA', 'AAC', 'UGC', 'CAA', 'GAC', 'CUA', 'GAU', 'AUU', 'AAA', 'UUG', 'UCA', 'GAA', 'AUC', 'UCU', 'AAU', 'ACU', 'UUA', 'UUG', 'GAU', 'AUA', 'GCA', 'AUG', 'GAC', 'CAA', 'ASN', 'UCU', 'UUC', 'CCC', 'UCC', 'AGU', 'UUC', 'AAA', 'UCA', 'LYS', 'TYR', 'UUG', 'AAU', 'AGC', 'AAU', 'LYS', 'ACA', 'UCU', 'UUA', 'GCA', 'AAA', 'UCU', 'CUC', 'UUA', 'GAC', 'GCA', 'UUA', 'CAA', 'ASN', 'UAU', 'UGU', 'AUU', 'CAA', 'UUG', 'LYS', 'CUG', 'GAA', 'GAA', 'UUC', 'UAC', 'CGU', 'UCA', 'CAA', 'GAA', 'UUG', 'GAA', 'ACC', 'AUG', 'UAC', 'AAU', 'ACU', 'ILE', 'UUU', 'GCU', 'CAG', 'UCC', 'UUU', 'GAC', 'AGC', 'GAU', 'UCA', 'UUG', 'ACU', 'UGU', 'GUU', 'UAC', 'UCA', 'AAU', 'GCU', 'ACU', 'ACU', 'CCA', 'AAG', 'AGC', 'GCU', 'ACG', 'GUU', 'UCA', 'UCU', 'ALA', 'GCC', 'ACA', 'ASP', 'UAU', 'PHE', 'SER', 'GAU', 'CAC', 'THR', 'CAU', 'UUA', 'AGA', 'ARG', 'UUG', 'THR', 'AAA', 'GAU', 'AGC', 'ILE', 'UCU', 'CCA', 'CCA', 'UUU', 'GCC', 'UUC', 'ACU', 'CCA', 'ACC', 'UCA', 'UCU', 'UCA', 'UCC', 'UCU', 'CCA', 'SER', 'CCA', 'UUC', 'AAU', 'UCC', 'CCU', 'UAC', 'LYS', 'ACU', 'UCA', 'AGU', 'UCA', 'AUG', 'THR', 'ACC', 'CCA', 'GAC', 'UCU', 'GCA', 'UCA', 'CAC', 'CAU', 'UCA', 'CAU', 'UCA', 'GGU', 'UCG', 'UUC', 'UCU', 'UCU', 'ACC', 'CAA', 'ASN', 'UCU', 'UUU', 'AAA', 'AGG', 'UCA', 'CUG', 'AGC', 'AUC', 'CCA', 'CAA', 'AAU', 'UCA', 'AGC', 'AUC', '[SEP]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCU', 'AGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CGG', 'GCC', '[PAD]', '[PAD]', '[PAD]', 'GCA', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', 'CAG', 'CCA', '[PAD]', '[PAD]', '[PAD]', 'CCG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', 'AGA', '[PAD]', '[PAD]', 'ACU', 'UUG', '[PAD]', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGC', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', 'AAA', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CGU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', 'UUG', 'GAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', 'UAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GCG', '[PAD]', '[PAD]', 'GAC', '[PAD]', 'UUC', 'UCG', '[PAD]', '[PAD]', 'ACU', '[PAD]', '[PAD]', '[PAD]', 'AGG', '[PAD]', 'ACC', '[PAD]', '[PAD]', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ACG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: -0.8830689787864685\n",
      "\n",
      "Example 10:\n",
      "Input Tokens: ['TE10', 'AUG', 'GGU', 'UGG', 'UUU', 'UGG', 'GCA', 'GAU', 'CAA', 'AAA', 'ACU', 'ACG', 'GGC', 'AAA', 'GAU', 'AUU', 'GGU', 'GGG', 'GCA', 'GCA', 'GUA', 'SER', 'UCC', 'AUG', 'UCA', 'GGG', 'UGC', 'CCA', 'GUC', 'AUG', 'CAC', 'GAG', 'UCG', 'UCG', 'UCG', 'UCG', 'UCG', 'CCA', 'CCA', 'UCC', 'UCU', 'GAG', 'UGC', 'PRO', 'GUU', 'AUG', 'CAG', 'GLY', 'GAU', 'ASN', 'GAU', 'AGA', 'AUA', 'ASN', 'PRO', 'CUG', 'AAC', 'AAU', 'MET', 'CCG', 'GLU', 'UUG', 'GCA', 'GCA', 'SER', 'AAA', 'GLN', 'CCU', 'GGC', 'GLN', 'AAG', 'AUG', 'GAC', 'UUG', 'PRO', 'GUU', 'GAU', 'CGG', 'ACC', 'ILE', 'UCC', 'AGC', 'AUC', 'CCC', 'AAG', 'AGU', 'CCA', 'GAC', 'AGU', 'ASN', 'GAG', 'PHE', 'UGG', 'GLU', 'UAU', 'CCU', 'UCU', 'CCA', 'CAA', 'CAG', 'AUG', 'UAC', 'AAU', 'GCU', 'AUG', 'VAL', 'ARG', 'AAG', 'GGC', 'AAG', 'AUU', 'GGC', 'GGU', 'AGC', 'GGC', 'GLU', 'GUC', 'GCC', 'GAA', 'GAU', 'GCA', 'GUG', 'GAG', 'UCC', 'AUG', 'GUG', 'CAG', 'GUC', 'CAC', 'AAC', 'UUU', 'CUA', 'AAU', 'GAA', 'GGG', 'CYS', 'UGG', 'GLN', 'GAA', 'GUG', 'CUC', 'GAA', 'UGG', 'GAA', 'AAA', 'CCG', 'CAC', 'ACA', 'GAU', 'GAA', 'AGC', 'CAC', 'GUG', 'CAG', 'CCU', 'AAG', 'UUG', 'CUG', 'AAA', 'PHE', 'AUG', 'GGG', 'AAA', 'CCG', 'GGC', 'GUA', 'UUG', 'SER', 'CCU', 'CGU', 'GCU', 'CGC', 'UGG', 'MET', 'CAC', 'CUG', 'UGC', 'GGC', 'CUA', 'LEU', 'UUU', 'CCG', 'UCC', 'HIS', 'UUU', 'AGC', 'CAA', 'GLU', 'CUA', 'CCA', 'UUC', 'ASP', 'AGG', 'CAC', 'ASP', 'UGG', 'ILE', 'GUA', 'CUC', 'CGA', 'GGC', 'GAG', 'CGC', 'AAA', 'GCG', 'GAA', 'CAA', 'CAA', 'CCU', 'CCA', 'ACC', 'UUC', 'AAG', 'GAA', 'GUU', 'AGA', 'UAC', 'GUC', 'UUG', 'GAU', 'UUC', 'UAC', 'GGA', 'GGG', 'CCC', 'GAC', 'ASP', 'GAA', 'AAC', 'GGA', 'AUG', 'CCU', 'ACU', 'UUC', 'CAC', 'GUG', 'GAU', 'VAL', 'CGU', 'CCU', 'GCC', 'CUA', 'ASP', 'AGU', 'CUA', 'GAC', 'AAU', 'GCU', 'AAG', 'GAC', 'CGG', 'MET', 'ACC', 'CGU', 'UUC', 'UUG', 'GAC', 'CGG', 'AUG', 'AUC', 'UCG', 'GGU', 'CCG', 'SER', 'UCU', 'UCG', 'UCC', 'UCC', 'GCC', 'CCU', 'UAA', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Label Tokens: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCC', '[PAD]', '[PAD]', '[PAD]', 'GGA', '[PAD]', 'AAC', '[PAD]', '[PAD]', '[PAD]', 'AAC', 'CCG', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', 'GAG', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', 'CAG', '[PAD]', '[PAD]', 'CAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AAC', '[PAD]', 'UUC', '[PAD]', 'GAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUU', 'AGA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UGC', '[PAD]', 'CAG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AGC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'CUG', '[PAD]', '[PAD]', '[PAD]', 'CAU', '[PAD]', '[PAD]', '[PAD]', 'GAA', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', 'GAC', '[PAD]', 'AUU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GUC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'GAU', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'AUG', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'UCC', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Expression Value: 1.5178580284118652\n"
     ]
    }
   ],
   "source": [
    "# Using the first two examples for testing\n",
    "mock_data = {\n",
    "    'sequence_tokens': df['sequence_tokens'].tolist()[:10],\n",
    "    'expression': df['expression'].tolist()[:10]\n",
    "}\n",
    "\n",
    "\n",
    "mock_dataset = Dataset.from_dict(mock_data)\n",
    "\n",
    "\n",
    "# Now test the custom_masking function\n",
    "masked_batch = custom_masking(mock_dataset, masking_prob_codons=0.1, masking_prob_TE=0)\n",
    "\n",
    "# Print the results\n",
    "for i in range(len(mock_dataset)):\n",
    "    input_ids = masked_batch['input_ids'][i]\n",
    "    labels = masked_batch['labels'][i]\n",
    "    expression = masked_batch['expression'][i]\n",
    "\n",
    "    # Convert IDs to tokens\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    label_ids = labels.numpy()\n",
    "    label_tokens = []\n",
    "    for label_id in label_ids:\n",
    "        if label_id == -100:\n",
    "            label_tokens.append('[PAD]')\n",
    "        else:\n",
    "            label_tokens.append(tokenizer.convert_ids_to_tokens([label_id])[0])\n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Input Tokens:\", input_tokens)\n",
    "    print(\"Label Tokens:\", label_tokens)\n",
    "    print(\"Expression Value:\", expression.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52cb7c80-989f-40f8-bc87-973d560062f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1732915248457,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "52cb7c80-989f-40f8-bc87-973d560062f2",
    "outputId": "95bb2323-88fd-43ea-8e22-00a90dac67c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins longer than 510 amino acids: 2040\n",
      "Total number of proteins: 5339\n",
      "Percentage of proteins longer than 510 amino acids: 38.21%\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_final is your DataFrame with 'sequence_tokens' column\n",
    "# Calculate the protein length (number of amino acids)\n",
    "df['protein_length'] = df['sequence_tokens'].apply(lambda x: len(x) - 1)  # Subtract 1 to exclude TE token\n",
    "\n",
    "\n",
    "# Number of proteins longer than 510 amino acids\n",
    "num_long_proteins = (df['protein_length'] > 510).sum()\n",
    "\n",
    "# Total number of proteins\n",
    "total_proteins = len(df)\n",
    "\n",
    "# Percentage of proteins longer than 510 amino acids\n",
    "percentage_long_proteins = (num_long_proteins / total_proteins) * 100\n",
    "\n",
    "print(f\"Number of proteins longer than 510 amino acids: {num_long_proteins}\")\n",
    "print(f\"Total number of proteins: {total_proteins}\")\n",
    "print(f\"Percentage of proteins longer than 510 amino acids: {percentage_long_proteins:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f1fab-6fdf-4ba5-89a4-dbc6b493ab60",
   "metadata": {
    "id": "959f1fab-6fdf-4ba5-89a4-dbc6b493ab60"
   },
   "source": [
    "### Define Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5aca047-e5ba-4082-aff3-14bb82bc5172",
   "metadata": {
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1732915250350,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "b5aca047-e5ba-4082-aff3-14bb82bc5172"
   },
   "outputs": [],
   "source": [
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, masking_prob_codons=0.15, masking_prob_TE=0.0, max_length=512):\n",
    "        super().__init__(tokenizer=tokenizer, padding=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.masking_prob_codons = masking_prob_codons\n",
    "        self.masking_prob_TE = masking_prob_TE\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def set_masking_probs(self, masking_prob_codons=None, masking_prob_TE=None):\n",
    "        if masking_prob_codons is not None:\n",
    "            self.masking_prob_codons = masking_prob_codons\n",
    "        if masking_prob_TE is not None:\n",
    "            self.masking_prob_TE = masking_prob_TE\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract sequences and expressions from features\n",
    "        sequences = [feature['sequence_tokens'] for feature in features]\n",
    "        expressions = [feature['expression'] for feature in features]\n",
    "        # Prepare batch for custom_masking function\n",
    "        batch = {'sequence_tokens': sequences, 'expression': expressions}\n",
    "        # Apply custom masking\n",
    "        masked_inputs = custom_masking(batch, self.masking_prob_codons, self.masking_prob_TE)\n",
    "        return masked_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e57048-5175-4d4f-a1c1-678f7a744d2b",
   "metadata": {
    "id": "d4e57048-5175-4d4f-a1c1-678f7a744d2b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fd68b-204a-4e61-be5d-1f8661ed6fa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1732915262615,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "886fd68b-204a-4e61-be5d-1f8661ed6fa6",
    "outputId": "3390b60c-d006-44cb-a5f4-05cbdd773f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{dir_path}/local_CodonBERT_Melina/ScCodonBERT/results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,  # Adjust the number of epochs as needed\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{dir_path}/local_CodonBERT_Melina/ScCodonBERT/logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False, #Very important\n",
    "    label_names=[\"labels\", \"expression\"], #This was sooooo important to get the custom_metrics to work\n",
    "    save_safetensors=False, #Not sure why, but not having this gives an error during training related to shared weights\n",
    "    # Add any additional parameters as needed\n",
    ")\n",
    "\n",
    "print(\"Training arguments defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a81d89-5fe2-4e7c-ba1e-dd12f74da7fd",
   "metadata": {
    "id": "52a81d89-5fe2-4e7c-ba1e-dd12f74da7fd"
   },
   "source": [
    "### Define Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "AeGQ9SlwOiNh",
   "metadata": {
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1732916496691,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "AeGQ9SlwOiNh"
   },
   "outputs": [],
   "source": [
    "#PLEASE CHECK: I'm calculating RMSE for the regression and perplexity for the MLM but are these the best metrics?\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"compute_metrics is called\")\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Unpack logits\n",
    "    prediction_scores = logits[0]  # MLM logits (batch_size, seq_len, vocab_size)\n",
    "    regression_outputs = logits[2]  # Regression logits (batch_size,)\n",
    "\n",
    "    # Unpack labels\n",
    "    labels_mlm = labels[0]  # MLM labels (batch_size, seq_len)\n",
    "    expressions = labels[1]  # Regression labels (batch_size,)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    prediction_scores = torch.tensor(prediction_scores)\n",
    "    labels_mlm = torch.tensor(labels_mlm)\n",
    "    regression_outputs = torch.tensor(regression_outputs)\n",
    "    expressions = torch.tensor(expressions)\n",
    "\n",
    "    # Compute MLM Loss and Perplexity\n",
    "    loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    vocab_size = prediction_scores.shape[-1]\n",
    "    mlm_loss = loss_fct(\n",
    "        prediction_scores.view(-1, vocab_size),  # Flatten logits\n",
    "        labels_mlm.view(-1)  # Flatten labels\n",
    "    )\n",
    "    perplexity = torch.exp(mlm_loss).item()  # Convert loss to perplexity\n",
    "    # Compute Accuracy\n",
    "    predicted_labels = torch.argmax(prediction_scores, dim=-1)\n",
    "    correct_predictions = (predicted_labels == labels_mlm).sum().item()\n",
    "    total_predictions = labels_mlm.numel()\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    # Compute Regression RMSE\n",
    "    regression_outputs = regression_outputs.view(-1)  # Ensure correct shape\n",
    "    expressions = expressions.view(-1)  # Ensure correct shape\n",
    "\n",
    "    regression_outputs_np = regression_outputs.detach().cpu().numpy()\n",
    "    expressions_np = expressions.detach().cpu().numpy()\n",
    "    mse = mean_squared_error(expressions_np, regression_outputs_np)\n",
    "    rmse = np.sqrt(mse)\n",
    "    metrics = {'eval_perplexity': perplexity, 'eval_rmse': rmse, 'eval_accuracy':accuracy}\n",
    "    # Return metrics with 'eval_' prefix for Trainer\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872475ac-c615-47a0-becf-b2d16d3d6310",
   "metadata": {
    "id": "872475ac-c615-47a0-becf-b2d16d3d6310"
   },
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d802ff0-e154-4730-8733-34c742e898eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1732916499311,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "2d802ff0-e154-4730-8733-34c742e898eb",
    "outputId": "013c130d-bd1e-4855-d546-e7809cce18af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-775df3089069>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom Data Collator\n",
    "data_collator = CustomDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    masking_prob_codons=0.0,  # Initial masking probability for codons\n",
    "    masking_prob_TE=0.0,       # Initial masking probability for TE tokens\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Example of updating masking probability\n",
    "#This is not used\n",
    "'''\n",
    "def adjust_masking_prob(trainer):\n",
    "    current_epoch = trainer.state.epoch\n",
    "    new_masking_prob = min(0.15 + current_epoch * 0.05, 0.5)  # Increase up to 50%\n",
    "    data_collator.set_masking_prob(new_masking_prob)\n",
    "'''\n",
    "# Add a callback to adjust masking probability\n",
    "#NEED TO IMPROVE (this is a very basic masking scheduler based on epoch, we should probably base it on performance)\n",
    "class MaskingProbCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        current_epoch = int(state.epoch or 0)\n",
    "        # Adjust masking probabilities based on the current epoch\n",
    "        new_masking_prob_codons = min(current_epoch * 0.1, 1)  # Enabled for all\n",
    "        new_masking_prob_TE = min(current_epoch * 0.02, 0)       # Disable for trainign on yeast, enable for fluorescent data\n",
    "        data_collator.set_masking_probs(\n",
    "            masking_prob_codons=new_masking_prob_codons,\n",
    "            masking_prob_TE=new_masking_prob_TE\n",
    "        )\n",
    "        print(f\"Epoch {current_epoch}: Masking probabilities set to codons={new_masking_prob_codons:.2f}, TE={new_masking_prob_TE:.2f}\")\n",
    "\n",
    "# Include the callback in the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MaskingProbCallback],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03cfd3-c26a-499d-a55a-dad0de5ca304",
   "metadata": {
    "id": "aa03cfd3-c26a-499d-a55a-dad0de5ca304",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2XhY9ArfUkSr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 191855,
     "status": "ok",
     "timestamp": 1732916693586,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "2XhY9ArfUkSr",
    "outputId": "10029748-8149-4ba9-ca41-f02e6b70bc3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_metrics is called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241129_214451-9pdliyno</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mmt515-new-york-university/huggingface/runs/9pdliyno' target=\"_blank\">/content/drive/MyDrive/local_CodonBERT_Melina/ScCodonBERT/results</a></strong> to <a href='https://wandb.ai/mmt515-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mmt515-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/mmt515-new-york-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mmt515-new-york-university/huggingface/runs/9pdliyno' target=\"_blank\">https://wandb.ai/mmt515-new-york-university/huggingface/runs/9pdliyno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results before training:\n",
      "eval_perplexity: nan\n",
      "eval_rmse: 1.908410668373108\n",
      "eval_accuracy: 0.0\n",
      "eval_loss: nan\n",
      "eval_model_preparation_time: 0.007\n",
      "eval_runtime: 16.2112\n",
      "eval_samples_per_second: 32.94\n",
      "eval_steps_per_second: 4.133\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model before training\n",
    "eval_results = trainer.evaluate(trainer.eval_dataset)\n",
    "\n",
    "print(\"Evaluation results before training:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dade2a6b-45d0-406f-b10e-6f0502f99e48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1732921162489,
     "user": {
      "displayName": "Melina Tsotras",
      "userId": "18029665946706776041"
     },
     "user_tz": 480
    },
    "id": "dade2a6b-45d0-406f-b10e-6f0502f99e48",
    "outputId": "eb634f40-cb59-4957-96d1-52beef64fc7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Masking probabilities set to codons=0.00, TE=0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 1:14:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.130552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.053300</td>\n",
       "      <td>4.566047</td>\n",
       "      <td>48.752125</td>\n",
       "      <td>0.825029</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.257100</td>\n",
       "      <td>2.444008</td>\n",
       "      <td>6.577800</td>\n",
       "      <td>0.748463</td>\n",
       "      <td>0.050931</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.927000</td>\n",
       "      <td>1.829803</td>\n",
       "      <td>3.949092</td>\n",
       "      <td>0.675447</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.552200</td>\n",
       "      <td>1.692621</td>\n",
       "      <td>3.310457</td>\n",
       "      <td>0.703916</td>\n",
       "      <td>0.139433</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.286500</td>\n",
       "      <td>1.605109</td>\n",
       "      <td>3.088480</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.178049</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.610900</td>\n",
       "      <td>1.513285</td>\n",
       "      <td>2.998774</td>\n",
       "      <td>0.644908</td>\n",
       "      <td>0.217927</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.281200</td>\n",
       "      <td>1.696516</td>\n",
       "      <td>2.945655</td>\n",
       "      <td>0.785381</td>\n",
       "      <td>0.256210</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.312900</td>\n",
       "      <td>1.564970</td>\n",
       "      <td>2.926046</td>\n",
       "      <td>0.701483</td>\n",
       "      <td>0.291729</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.378200</td>\n",
       "      <td>1.576689</td>\n",
       "      <td>2.942482</td>\n",
       "      <td>0.705633</td>\n",
       "      <td>0.326377</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='134' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 10:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_metrics is called\n",
      "Epoch 1: Masking probabilities set to codons=0.10, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 2: Masking probabilities set to codons=0.20, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 3: Masking probabilities set to codons=0.30, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 4: Masking probabilities set to codons=0.40, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 5: Masking probabilities set to codons=0.50, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 6: Masking probabilities set to codons=0.60, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 7: Masking probabilities set to codons=0.70, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 8: Masking probabilities set to codons=0.80, TE=0.00\n",
      "compute_metrics is called\n",
      "Epoch 9: Masking probabilities set to codons=0.90, TE=0.00\n",
      "compute_metrics is called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5340, training_loss=2.1344891794611898, metrics={'train_runtime': 4447.4929, 'train_samples_per_second': 9.603, 'train_steps_per_second': 1.201, 'total_flos': 1.132545486692352e+16, 'train_loss': 2.1344891794611898, 'epoch': 10.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f67d0-c841-42be-93ef-cad71271da30",
   "metadata": {
    "id": "571f67d0-c841-42be-93ef-cad71271da30",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c0235-af5c-44e3-8969-00f381831806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "executionInfo": {
     "elapsed": 5404,
     "status": "ok",
     "timestamp": 1732509195999,
     "user": {
      "displayName": "Antonio Vela G.",
      "userId": "02403341420005464923"
     },
     "user_tz": 300
    },
    "id": "d60c0235-af5c-44e3-8969-00f381831806",
    "outputId": "e21ef9ab-1382-4c70-e31c-8b96adbb5890"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_metrics is called\n",
      "Evaluation results: {'eval_perplexity': 109.70259094238281, 'eval_rmse': 1.4294681549072266, 'eval_loss': 6.742545127868652, 'eval_runtime': 5.4583, 'eval_samples_per_second': 97.833, 'eval_steps_per_second': 12.275, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#NEED TO CHECK\n",
    "#Not sure what level of masking this cell is doing\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d311f70-d188-4e0f-98ed-6d1c0cde6b89",
   "metadata": {
    "id": "8d311f70-d188-4e0f-98ed-6d1c0cde6b89",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0835181-8e94-4ef6-ab00-2ba4d01f249d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1732509270365,
     "user": {
      "displayName": "Antonio Vela G.",
      "userId": "02403341420005464923"
     },
     "user_tz": 300
    },
    "id": "d0835181-8e94-4ef6-ab00-2ba4d01f249d",
    "outputId": "31a29780-14b0-47ae-93b6-4128ce024eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to /content/drive/MyDrive/local_CodonBERT/ScCodonBERT2/TE_fine_tuned_models\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model_path = f'{dir_path}/local_CodonBERT_Melina/ScCodonBERT/TE_fine_tuned_models'\n",
    "trainer.save_model(fine_tuned_model_path)\n",
    "tokenizer.save_pretrained(fine_tuned_model_path)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ed2d1-466f-4e78-82fc-7521449a3b1f",
   "metadata": {
    "id": "c26ed2d1-466f-4e78-82fc-7521449a3b1f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test the Model on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09e662-077a-4507-88c1-3f7eb2fe19e2",
   "metadata": {
    "id": "af09e662-077a-4507-88c1-3f7eb2fe19e2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
